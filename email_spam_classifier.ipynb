{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af601dc",
   "metadata": {},
   "source": [
    "Assignment: Classification\n",
    "Classification refers to categorizing the given data into classes. For example,\n",
    "\n",
    "Given an image of hand-written character, identifying the character (multi-class classification)\n",
    "Given an image, annotating it with all the objects present in the image (multi-label classification)\n",
    "Classifying an email as spam or non-spam (binary classification)\n",
    "Classifying a tumor as benign or malignant and so on\n",
    "In this assignment, we will be building a classifier to classify emails as spam or non-spam. We will be using the Kaggle dataset Spam or Not Spam Dataset for this task.\n",
    "\n",
    "Note: You cannot load any libraries other than the mentioned ones.\n",
    "\n",
    "Data pre-processing\n",
    "The first step in every machine learning algorithm is to process the raw data in some meaningful representations. We will be using the Bag-of-Words representation to process the text. It comprises of following steps:\n",
    "\n",
    "Process emails line-by-line to extract all the words.\n",
    "Replace extracted words by their stem (root) word. This is known as stemming and lematization.\n",
    "Remove stop words like and, or, is, am, and so on.\n",
    "Assign a unique index to each word. This forms the vocabulary.\n",
    "Represent each email as a binary vector of length equal to the size of the vocabulary such that the \n",
    " element of the vector is 1 iff the \n",
    " word is present in the email.\n",
    "Here we provide you with the function signature along with the expected functionality. You are expected to complete them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b934b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e57ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes an email as an argument\n",
    "# read email line-by-line and extract all the words\n",
    "# return list of extracted words\n",
    "def read_email(email):\n",
    "  list_of_words=email.split(\" \")\n",
    "  return list_of_words\n",
    "  \n",
    "# takes a list of words as an argument\n",
    "# replace each word by their stem word\n",
    "# return list of stem words\n",
    "def stemming(list_of_words):\n",
    "  list_of_stem_words=[]\n",
    "  ps= PorterStemmer()\n",
    "  for word in list_of_words:\n",
    "    list_of_stem_words.append(ps.stem(word))\n",
    "  return list_of_stem_words\n",
    "\n",
    "# takes a list of stem-words as an argument\n",
    "# remove stop words\n",
    "# return list of stem words after removing stop words\n",
    "def remove_stop_words(list_of_stem_words):\n",
    "  stem_no_stop_words=[]\n",
    "  list_of_stop_words=set(stopwords.words('english'))\n",
    "  for word in list_of_stem_words:\n",
    "    if word not in list_of_stop_words:\n",
    "      stem_no_stop_words.append(word)\n",
    "  return stem_no_stop_words\n",
    "\n",
    "# takes a list of stem-words as an argument\n",
    "# add new words to the vocabulary and assign a unique index to them\n",
    "# returns new vocabulary\n",
    "def build_vocabulary(stem_no_stop_words):\n",
    "  vocab=[]\n",
    "  for list in stem_no_stop_words:\n",
    "    for word in list:\n",
    "      if word not in vocab:\n",
    "        vocab.append(word)\n",
    "  vocab.pop(0)\n",
    "  return vocab\n",
    "\n",
    "# takes a list of stem-words and vocabulary as an argument\n",
    "# returns bow representation\n",
    "def get_bow(stem_no_stop_words,vocab):\n",
    "  email_bow=[]\n",
    "  for list in stem_no_stop_words:\n",
    "    element_email_bow=[]\n",
    "    for word in vocab:\n",
    "      if word in list:\n",
    "        element_email_bow.append(1)\n",
    "      else:\n",
    "        element_email_bow.append(0)\n",
    "    email_bow.append(element_email_bow)\n",
    "  return email_bow\n",
    "\n",
    "# read the entire dataset\n",
    "# convert emails to bow and maintain their labels\n",
    "# call function text_to_bow()\n",
    "def read_data():\n",
    "  list_of_lines = []\n",
    "  with open('spam_or_not_spam.csv', 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "                elements = line.split(',')\n",
    "                list_of_lines.append([elements[0],elements[1][0][0]])\n",
    "  list_of_lines.pop(0)\n",
    "  list_of_stem_words = []\n",
    "  dataset= []\n",
    "  for line in list_of_lines:\n",
    "    r= read_email(line[0])\n",
    "    s=stemming(r)\n",
    "    f=remove_stop_words(s)\n",
    "    list_of_stem_words.append(f)\n",
    "  vocab= build_vocabulary(list_of_stem_words)\n",
    "  email_bow = get_bow(list_of_stem_words,vocab)\n",
    "  n = len(list_of_lines)\n",
    "  for i in range(0,n):\n",
    "    dataset.append([email_bow[i],int(list_of_lines[i][1])])\n",
    "  return dataset, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafadbe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data,vocab\u001b[38;5;241m=\u001b[39m\u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocab)\n",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m   r\u001b[38;5;241m=\u001b[39m read_email(line[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     69\u001b[0m   s\u001b[38;5;241m=\u001b[39mstemming(r)\n\u001b[1;32m---> 70\u001b[0m   f\u001b[38;5;241m=\u001b[39m\u001b[43mremove_stop_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m   list_of_stem_words\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m     72\u001b[0m vocab\u001b[38;5;241m=\u001b[39m build_vocabulary(list_of_stem_words)\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mremove_stop_words\u001b[1;34m(list_of_stem_words)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stop_words\u001b[39m(list_of_stem_words):\n\u001b[0;32m     22\u001b[0m   stem_no_stop_words\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m---> 23\u001b[0m   list_of_stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m list_of_stem_words:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m list_of_stop_words:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data,vocab=read_data()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ee3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuallze data distribution\n",
    "def data_vis(dataset, vocab):\n",
    "  list_of_all_words=[0 for word in vocab]\n",
    "  list_of_spam=[0 for word in vocab]\n",
    "  list_of_non_spam=[0 for word in vocab]\n",
    "\n",
    "  # print(dataset)\n",
    "  for line in dataset:\n",
    "    for i in range(0,len(line[0])):\n",
    "      if line[0][i] == 1:\n",
    "        list_of_all_words[i]+=1\n",
    "        if line[1]==0:\n",
    "          list_of_non_spam[i]+=1\n",
    "        else:\n",
    "          list_of_spam[i]+=1\n",
    "\n",
    "  plt.bar(vocab, list_of_all_words, color = \"purple\")\n",
    "  plt.title(\"Plot for Spam and Non-Spam emails\")\n",
    "  plt.show()\n",
    "\n",
    "  plt.bar(vocab, list_of_spam, color = \"purple\")\n",
    "  plt.title(\"Plot for Non-Spam emails\")\n",
    "  plt.show()\n",
    "\n",
    "  plt.bar(vocab, list_of_non_spam, color = \"purple\")\n",
    "  plt.title(\"Plot for Spam emails\")\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "data_vis(data,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "def split(dataset):\n",
    "  np.random.shuffle(dataset)\n",
    "  separation_index= int(len(dataset)*8/10)\n",
    "  train_dataset=[]\n",
    "  test_dataset=[]\n",
    "  index=0\n",
    "  for data in dataset:\n",
    "    if index < separation_index:\n",
    "      train_dataset.append(data)\n",
    "    else:\n",
    "      test_dataset.append(data)\n",
    "    index=index+1\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "# learn a SVM model\n",
    "# use the model to make prediction\n",
    "# return the model predictions on train and test dataset\n",
    "def svm_classifier(train_dataset, test_dataset):\n",
    "  train_data=[]\n",
    "  train_label=[]\n",
    "  test_data=[]\n",
    "  test_label=[]\n",
    "\n",
    "  for data in train_dataset:\n",
    "    train_data.append(data[0])\n",
    "    train_label.append(data[1])\n",
    "\n",
    "  for data in test_dataset:\n",
    "    test_data.append(data[0])\n",
    "    test_label.append(data[1])\n",
    "\n",
    "  clf = svm.SVC()\n",
    "  clf.fit(train_data,train_label)\n",
    "\n",
    "  svm_train_predictions= clf.predict(train_data)\n",
    "  svm_test_predictions = clf.predict(test_data)\n",
    "\n",
    "  return svm_train_predictions, svm_test_predictions\n",
    "\n",
    "# implement k-NN algorithm\n",
    "# use the model to make prediction\n",
    "# return the model predictions on train and test dataset\n",
    "def knn_classifier(train_dataset, test_dataset):\n",
    "  train_data=[]\n",
    "  train_label=[]\n",
    "  test_data=[]\n",
    "  test_label=[]\n",
    "\n",
    "  for data in train_dataset:\n",
    "    train_data.append(data[0])\n",
    "    train_label.append(data[1])\n",
    "\n",
    "  for data in test_dataset:\n",
    "    test_data.append(data[0])\n",
    "    test_label.append(data[1])\n",
    "\n",
    "  knn= KNeighborsClassifier(n_neighbors=3)\n",
    "  knn.fit(train_data,train_label)\n",
    "\n",
    "  knn_train_predictions= knn.predict(train_data)\n",
    "  knn_test_predictions = knn.predict(test_data)\n",
    "\n",
    "  return knn_train_predictions, knn_test_predictions\n",
    "\n",
    "train_data, test_data = split(data)\n",
    "svm_train_predictions, svm_test_predictions = svm_classifier(train_data, test_data)\n",
    "knn_train_predictions, knn_test_predictions = knn_classifier(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy \n",
    "def compute_accuracy(true_labels, predicted_labels):\n",
    "  accuracy= metrics.accuracy_score(true_labels,predicted_labels)\n",
    "  return accuracy\n",
    "\n",
    "# compute AUC score \n",
    "def compute_auc(true_labels, predicted_labels):\n",
    "  false_positive_rate,true_positive_rate,threshold = metrics.roc_curve(true_labels,predicted_labels)\n",
    "  auc_score=metrics.auc(false_positive_rate,true_positive_rate)\n",
    "  return auc_score\n",
    "\n",
    "# write code to print train and test accuracy and AUC score of SVM and k-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e00dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = split(data)\n",
    "svm_train_predictions,svm_test_predictions=svm_classifier(train_dataset, test_dataset)\n",
    "knn_train_predictions,knn_test_predictions=knn_classifier(train_dataset, test_dataset)\n",
    "\n",
    "true_train_label=[]\n",
    "true_test_label=[]\n",
    "for data in train_dataset:\n",
    "    true_train_label.append(data[1])\n",
    "\n",
    "for data in test_dataset:\n",
    "    true_test_label.append(data[1])\n",
    "\n",
    "print(\"SVM Classifier\")\n",
    "print(\"Train accuracy: \",compute_accuracy(true_train_label,svm_train_predictions))\n",
    "print(\"Train auc_score: \",compute_auc(true_train_label,svm_train_predictions))\n",
    "print(\"Test accuracy: \",compute_accuracy(true_test_label,svm_test_predictions))\n",
    "print(\"Test auc_score: \",compute_auc(true_test_label,svm_test_predictions))\n",
    "\n",
    "print(\"KNN Classifier\")\n",
    "print(\"Train accuracy: \",compute_accuracy(true_train_label,knn_train_predictions))\n",
    "print(\"Train auc_score: \",compute_auc(true_train_label,knn_train_predictions))\n",
    "print(\"Test accuracy: \",compute_accuracy(true_test_label,knn_test_predictions))\n",
    "print(\"Test auc_score: \",compute_auc(true_test_label,knn_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531cbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
